{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "model_name = 'skt/ko-gpt-trinity-1.2B-v0.5'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model_config = AutoConfig.from_pretrained(model_name)\n",
    "#model_config.num_labels = 30"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, config=model_config)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import ast\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "data_dir = '../dataset/train/train.csv'\n",
    "data = pd.read_csv(data_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def add_entity_tokens(sentence, object_entity, subject_entity):\n",
    "    def entity_mapper(entity_type):\n",
    "        e_map = {'PER' : '인물', 'ORG' : '기관', 'LOC' : '지명', 'POH' : '기타', 'DAT' : '날짜', 'NOH' : '수량'}\n",
    "        return e_map[entity_type]\n",
    "    def extract(entity): return int(ast.literal_eval(entity)['start_idx']), int(ast.literal_eval(entity)['end_idx']), entity_mapper(ast.literal_eval(entity)['type'])\n",
    "    obj_start_idx, obj_end_idx, obj_type = extract(object_entity)\n",
    "    subj_start_idx, subj_end_idx, sbj_type = extract(subject_entity)\n",
    "    \n",
    "    if obj_start_idx < subj_start_idx:\n",
    "        new_sentence = sentence[:obj_start_idx] + '#' + '+' + obj_type + '+' + sentence[obj_start_idx:obj_end_idx+1] + '#' + \\\n",
    "                       sentence[obj_end_idx+1:subj_start_idx] + '@' + '^' + sbj_type + '^' + sentence[subj_start_idx:subj_end_idx+1] + \\\n",
    "                       '@' + sentence[subj_end_idx+1:]\n",
    "    else:\n",
    "        new_sentence = sentence[:subj_start_idx] + '@' + '^' + sbj_type + '^' + sentence[subj_start_idx:subj_end_idx+1] + '@' + \\\n",
    "                       sentence[subj_end_idx+1:obj_start_idx] + '#' + '+' + obj_type + '+' + sentence[obj_start_idx:obj_end_idx+1] + \\\n",
    "                       '#' + sentence[obj_end_idx+1:]\n",
    "    \n",
    "    return new_sentence\n",
    "\n",
    "\n",
    "def ent_preprocess(data):\n",
    "    data['sentence'] = data.apply(lambda row: add_entity_tokens(row['sentence'], row['object_entity'], row['subject_entity']), axis=1)\n",
    "    return data\n",
    "\n",
    "data = ent_preprocess(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "ex_sentence = data['sentence'][41]\n",
    "ex_sentence2 = data['sentence'][20]\n",
    "ex_encoding = tokenizer(ex_sentence,\n",
    "                max_length=64,\n",
    "                padding='max_length',\n",
    "                truncation=True)\n",
    "ex_encoding2 = tokenizer(ex_sentence2,\n",
    "                max_length=64,\n",
    "                padding='max_length',\n",
    "                truncation=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "print(tokenizer.decode(ex_encoding['input_ids']))\n",
    "print(ex_encoding[\"input_ids\"])\n",
    "print(ex_sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "이 캐릭터가 작가 스탠 리와 만화가 잭 커비가 만든 \"\" 에서 캡틴 아메리카의 전쟁 시절 애정 상대로 처음으로 등장하였을 당시에는 이름이 없었다.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "[29976, 40658, 32603, 37329, 30284, 25512, 30048, 31625, 36523, 30582, 32420, 31210, 30013, 378, 32162, 49671, 40738, 25792, 31110, 31487, 49112, 33175, 32770, 30672, 46009, 38371, 33736, 31545, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "이 캐릭터가 작가 스탠 리와 만화가 잭 커비가 만든 \"\" 에서 캡틴 아메리카의 전쟁 시절 애정 상대로 처음으로 등장하였을 당시에는 이름이 없었다.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "print(tokenizer.decode(ex_encoding2['input_ids']))\n",
    "print(ex_encoding2[\"input_ids\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1971년 대선을 앞두고 김종필은 1971년 선거에서 박정희 당선을 위해 무려 600억원이나 썼다고 밝혔다.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "[46252, 29983, 31097, 34123, 34828, 29356, 25768, 46252, 41040, 39789, 30087, 31097, 30234, 34519, 40390, 31523, 30195, 33251, 30062, 32733, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "stc = \"인물 지명 기관 기타 수량 날짜 나머지 단체 사람\"\n",
    "ex_encoding = tokenizer(stc,\n",
    "                max_length=20,\n",
    "                padding='max_length',\n",
    "                truncation=True)\n",
    "print(ex_encoding['input_ids'])\n",
    "print(print(tokenizer.decode(ex_encoding['input_ids'])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[31169, 36514, 32043, 32112, 29985, 21809, 39231, 32960, 32964, 30086, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "인물 지명 기관 기타 수량 날짜 나머지 단체 사람<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}