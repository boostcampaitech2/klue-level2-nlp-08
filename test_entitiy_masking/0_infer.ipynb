{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import argparse\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from custom_model import RBERT\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "\n",
    "helper = DataHelper(data_dir='/opt/ml/dataset/test/preprocess_test.csv',mode='inference')\n",
    "_test_data = helper.from_idxs()\n",
    "test_data = helper.entity_tokenize(data=_test_data, tokenizer=tokenizer)\n",
    "test_dataset = RelationExtractionDataset(test_data)\n",
    "\n",
    "probs = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokenizer.tokenize('ㄱ')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['ㄱ']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model_config = AutoConfig.from_pretrained('klue/roberta-large', num_labels=30)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('klue/roberta-large')\n",
    "dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "preds, probs = [], []\n",
    "model = RBERT.from_pretrained('/opt/ml/klue-level2-nlp-08/test_entitiy_masking/best_model/'+str(1)+'_fold/',config=model_config, model_name='klue/roberta-large', dropout_rate=0.1)\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "model.label_classifier.linear.weight"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0027, -0.0048, -0.0126,  ..., -0.0080,  0.0093, -0.0036],\n",
       "        [-0.0161,  0.0107, -0.0102,  ..., -0.0055, -0.0086,  0.0049],\n",
       "        [ 0.0041,  0.0151,  0.0111,  ...,  0.0086, -0.0007,  0.0061],\n",
       "        ...,\n",
       "        [ 0.0093,  0.0109,  0.0060,  ..., -0.0141, -0.0094,  0.0151],\n",
       "        [-0.0002, -0.0090, -0.0190,  ...,  0.0167, -0.0093, -0.0182],\n",
       "        [-0.0101,  0.0039,  0.0051,  ...,  0.0073,  0.0149, -0.0024]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = RBERT.from_pretrained('/opt/ml/klue-level2-nlp-08/test_entitiy_masking/best_model/'+str(1)+'_fold/',config=model_config, model_name='klue/roberta-large', dropout_rate=0.1)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "model.label_classifier.linear.weight"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0027, -0.0048, -0.0126,  ..., -0.0080,  0.0093, -0.0036],\n",
       "        [-0.0161,  0.0107, -0.0102,  ..., -0.0055, -0.0086,  0.0049],\n",
       "        [ 0.0041,  0.0151,  0.0111,  ...,  0.0086, -0.0007,  0.0061],\n",
       "        ...,\n",
       "        [ 0.0093,  0.0109,  0.0060,  ..., -0.0141, -0.0094,  0.0151],\n",
       "        [-0.0002, -0.0090, -0.0190,  ...,  0.0167, -0.0093, -0.0182],\n",
       "        [-0.0101,  0.0039,  0.0051,  ...,  0.0073,  0.0149, -0.0024]],\n",
       "       requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "test_dataset[0]\n",
    "next(iter(dataloader))['input_ids']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0]),\n",
       " tensor([3625, 3784, 3995,   36]),\n",
       " tensor([ 3749, 21154,    16,    14]),\n",
       " tensor([2210,    7, 5006, 3611]),\n",
       " tensor([    7,    14, 16576,    14]),\n",
       " tensor([  14, 3611, 2283, 1543]),\n",
       " tensor([3971,   14, 1498, 2335]),\n",
       " tensor([  14, 8354, 4347, 2390]),\n",
       " tensor([ 5633, 24141,  2200,    36]),\n",
       " tensor([   7, 1878, 1891, 1497]),\n",
       " tensor([  36, 2557, 5633,  648]),\n",
       " tensor([   14,     7, 10914,  2431]),\n",
       " tensor([ 3971,  1503, 16460,  2145]),\n",
       " tensor([  14, 3934,  543, 3848]),\n",
       " tensor([11428,  2073,    36, 29128]),\n",
       " tensor([26648,  1165,    14,  2200]),\n",
       " tensor([27967,  2079,  9384, 10207]),\n",
       " tensor([  36, 6717,   14,  594]),\n",
       " tensor([ 543, 2098, 7394, 2447]),\n",
       " tensor([ 1504, 30698,  2440,  2205]),\n",
       " tensor([3669, 6104,   36, 2307]),\n",
       " tensor([2069,   86,   26, 1327]),\n",
       " tensor([4256, 8164,   18, 2073]),\n",
       " tensor([ 2371,  4586,  3912, 11894]),\n",
       " tensor([ 2062,  2052, 10914,  3926]),\n",
       " tensor([  18, 2241, 2216, 2088]),\n",
       " tensor([   2,  936, 6233,  648]),\n",
       " tensor([   1, 6233, 6201, 2431]),\n",
       " tensor([   1, 5632, 2651, 2154]),\n",
       " tensor([   1,   16, 3834, 4638]),\n",
       " tensor([   1, 6973, 2052, 3844]),\n",
       " tensor([   1,   16, 2359, 2205]),\n",
       " tensor([    1,  9300, 13964,  2507]),\n",
       " tensor([    1,   886, 15608, 13964]),\n",
       " tensor([   1, 3789, 3635,   16]),\n",
       " tensor([   1, 3663, 3624,  648]),\n",
       " tensor([   1, 3805, 2170, 2431]),\n",
       " tensor([    1,  2138, 26279,  2079]),\n",
       " tensor([    1, 15112,   594,  1562]),\n",
       " tensor([   1, 8021, 2055, 2259]),\n",
       " tensor([   1, 2305, 2154, 1038]),\n",
       " tensor([   1, 2226, 7432, 2517]),\n",
       " tensor([   1, 2079, 3817, 1565]),\n",
       " tensor([   1,   36,  812, 2051]),\n",
       " tensor([   1,   14, 2395, 7213]),\n",
       " tensor([    1,  5064,  2216, 11800]),\n",
       " tensor([   1,   14, 5680,   18]),\n",
       " tensor([   1, 6717,  543, 3609]),\n",
       " tensor([   1,   36, 5633, 1889]),\n",
       " tensor([   1,  805, 2170, 2209]),\n",
       " tensor([   1, 2079, 3691,   16]),\n",
       " tensor([   1, 3934, 2069,    7]),\n",
       " tensor([   1, 2052, 4698,   14]),\n",
       " tensor([   1,  859, 2371, 3611]),\n",
       " tensor([   1, 2359, 3683,   14]),\n",
       " tensor([    1,  2062,  7236, 21396]),\n",
       " tensor([   1,   18, 2481,    7]),\n",
       " tensor([   1,    2, 2470,  543]),\n",
       " tensor([   1,    1,  842, 4014]),\n",
       " tensor([   1,    1, 2062,  648]),\n",
       " tensor([   1,    1, 5633, 2431]),\n",
       " tensor([   1,    1, 2116, 2069]),\n",
       " tensor([    1,     1,     7, 10601]),\n",
       " tensor([   1,    1,   14, 2205]),\n",
       " tensor([   1,    1, 3971, 2507]),\n",
       " tensor([   1,    1,   14, 2062]),\n",
       " tensor([   1,    1, 3858,   18]),\n",
       " tensor([1, 1, 7, 2]),\n",
       " tensor([  1,   1, 604,   1]),\n",
       " tensor([   1,    1, 2079,    1]),\n",
       " tensor([   1,    1, 3654,    1]),\n",
       " tensor([   1,    1, 9104,    1]),\n",
       " tensor([   1,    1, 2138,    1]),\n",
       " tensor([   1,    1, 8461,    1]),\n",
       " tensor([   1,    1, 7488,    1]),\n",
       " tensor([   1,    1, 4271,    1]),\n",
       " tensor([   1,    1, 2069,    1]),\n",
       " tensor([    1,     1, 30573,     1]),\n",
       " tensor([   1,    1, 2118,    1]),\n",
       " tensor([   1,    1, 1380,    1]),\n",
       " tensor([   1,    1, 2318,    1]),\n",
       " tensor([    1,     1, 11469,     1]),\n",
       " tensor([   1,    1, 2088,    1]),\n",
       " tensor([   1,    1, 1504,    1]),\n",
       " tensor([   1,    1, 3747,    1]),\n",
       " tensor([    1,     1, 27135,     1]),\n",
       " tensor([    1,     1, 26279,     1]),\n",
       " tensor([   1,    1, 2116,    1]),\n",
       " tensor([   1,    1, 7394,    1]),\n",
       " tensor([   1,    1, 2440,    1]),\n",
       " tensor([ 1,  1, 23,  1]),\n",
       " tensor([   1,    1, 2429,    1]),\n",
       " tensor([   1,    1, 4346,    1]),\n",
       " tensor([   1,    1, 2210,    1]),\n",
       " tensor([   1,    1, 5633,    1]),\n",
       " tensor([   1,    1, 2170,    1]),\n",
       " tensor([    1,     1, 13242,     1]),\n",
       " tensor([   1,    1, 2138,    1]),\n",
       " tensor([   1,    1, 4962,    1]),\n",
       " tensor([   1,    1, 2470,    1]),\n",
       " tensor([   1,    1, 1943,    1]),\n",
       " tensor([    1,     1, 24988,     1]),\n",
       " tensor([   1,    1, 2138,    1]),\n",
       " tensor([   1,    1, 4838,    1]),\n",
       " tensor([   1,    1, 7488,    1]),\n",
       " tensor([   1,    1, 6201,    1]),\n",
       " tensor([   1,    1, 2052,    1]),\n",
       " tensor([   1,    1, 8277,    1]),\n",
       " tensor([   1,    1, 2367,    1]),\n",
       " tensor([   1,    1, 4007,    1]),\n",
       " tensor([  1,   1, 594,   1]),\n",
       " tensor([   1,    1, 2055,    1]),\n",
       " tensor([   1,    1, 2154,    1]),\n",
       " tensor([   1,    1, 7432,    1]),\n",
       " tensor([   1,    1, 2259,    1]),\n",
       " tensor([   1,    1, 1503,    1]),\n",
       " tensor([   1,    1, 4271,    1]),\n",
       " tensor([   1,    1, 2145,    1]),\n",
       " tensor([   1,    1, 3700,    1]),\n",
       " tensor([   1,    1, 2470,    1]),\n",
       " tensor([   1,    1, 5633,    1]),\n",
       " tensor([   1,    1, 2522,    1]),\n",
       " tensor([   1,    1, 2079,    1]),\n",
       " tensor([    1,     1, 11342,     1]),\n",
       " tensor([   1,    1, 1462,    1]),\n",
       " tensor([   1,    1, 6509,    1]),\n",
       " tensor([   1,    1, 7756,    1]),\n",
       " tensor([   1,    1, 2440,    1]),\n",
       " tensor([   1,    1, 3633,    1]),\n",
       " tensor([   1,    1, 2429,    1]),\n",
       " tensor([1, 1, 6, 1]),\n",
       " tensor([   1,    1, 1860,    1]),\n",
       " tensor([    1,     1, 17287,     1]),\n",
       " tensor([    1,     1, 16093,     1]),\n",
       " tensor([   1,    1, 2200,    1]),\n",
       " tensor([   1,    1, 1507,    1]),\n",
       " tensor([    1,     1, 19521,     1]),\n",
       " tensor([   1,    1, 1335,    1]),\n",
       " tensor([   1,    1, 2062,    1]),\n",
       " tensor([1, 1, 6, 1]),\n",
       " tensor([   1,    1, 1072,    1]),\n",
       " tensor([   1,    1, 3817,    1]),\n",
       " tensor([   1,    1, 1048,    1]),\n",
       " tensor([   1,    1, 2088,    1]),\n",
       " tensor([   1,    1, 1513,    1]),\n",
       " tensor([   1,    1, 2414,    1]),\n",
       " tensor([  1,   1, 812,   1]),\n",
       " tensor([   1,    1, 2395,    1]),\n",
       " tensor([   1,    1, 2216,    1]),\n",
       " tensor([   1,    1, 3998,    1]),\n",
       " tensor([   1,    1, 2325,    1]),\n",
       " tensor([   1,    1, 2134,    1]),\n",
       " tensor([   1,    1, 4339,    1]),\n",
       " tensor([    1,     1, 12255,     1]),\n",
       " tensor([   1,    1, 6468,    1]),\n",
       " tensor([    1,     1, 28714,     1]),\n",
       " tensor([   1,    1, 9681,    1]),\n",
       " tensor([    1,     1, 14625,     1]),\n",
       " tensor([   1,    1, 6336,    1]),\n",
       " tensor([    1,     1, 12733,     1]),\n",
       " tensor([   1,    1, 2138,    1]),\n",
       " tensor([   1,    1, 3705,    1]),\n",
       " tensor([   1,    1, 3683,    1]),\n",
       " tensor([   1,    1, 5633,    1]),\n",
       " tensor([   1,    1, 1686,    1]),\n",
       " tensor([   1,    1, 2079,    1]),\n",
       " tensor([   1,    1, 4138,    1]),\n",
       " tensor([   1,    1, 2200,    1]),\n",
       " tensor([   1,    1, 8277,    1]),\n",
       " tensor([   1,    1, 2496,    1]),\n",
       " tensor([   1,    1, 2155,    1]),\n",
       " tensor([    1,     1, 24988,     1]),\n",
       " tensor([   1,    1, 2138,    1]),\n",
       " tensor([   1,    1, 4838,    1]),\n",
       " tensor([   1,    1, 2371,    1]),\n",
       " tensor([   1,    1, 2062,    1]),\n",
       " tensor([ 1,  1, 18,  1]),\n",
       " tensor([1, 1, 2, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1]),\n",
       " tensor([1, 1, 1, 1])]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for data in tqdm(dataloader):\n",
    "    batch = {k: v.to(device) for k, v in data.items()}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/3883 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7064ff9f003c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-7064ff9f003c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}